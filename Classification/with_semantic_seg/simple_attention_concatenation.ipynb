{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4989cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Ewa_loading_data_scene_kinds import MultiPartitioningClassifier\n",
    "import yaml\n",
    "from argparse import Namespace\n",
    "import torch\n",
    "\n",
    "with open('../config/baseM_Ewa.yml') as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    \n",
    "model_params = config[\"model_params\"]\n",
    "tmp_model = MultiPartitioningClassifier(hparams=Namespace(**model_params))\n",
    "\n",
    "train_data_loader = tmp_model.train_dataloader()\n",
    "val_data_loader = tmp_model.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2ae6133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 224, 224, 3])\n",
      "torch.Size([16, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "it = iter(val_data_loader)\n",
    "first_batch = next(it)\n",
    "print(first_batch[1].shape)\n",
    "print(torch.permute(first_batch[1], (0,3,1,2)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64c82de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cis/home/shraman/glimpse/classification_network/glimpse_classification_env/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py:30: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 0 which\n",
      "    has less than 75% of the memory or cores of GPU 1. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "            Conv2d-2         [-1, 64, 112, 112]           9,408\n",
      "            Conv2d-3         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-4         [-1, 64, 112, 112]             128\n",
      "            Conv2d-5         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-6         [-1, 64, 112, 112]             128\n",
      "              ReLU-7         [-1, 64, 112, 112]               0\n",
      "              ReLU-8         [-1, 64, 112, 112]               0\n",
      "       BatchNorm2d-9         [-1, 64, 112, 112]             128\n",
      "        MaxPool2d-10           [-1, 64, 56, 56]               0\n",
      "        MaxPool2d-11           [-1, 64, 56, 56]               0\n",
      "      BatchNorm2d-12         [-1, 64, 112, 112]             128\n",
      "             ReLU-13         [-1, 64, 112, 112]               0\n",
      "             ReLU-14         [-1, 64, 112, 112]               0\n",
      "           Conv2d-15           [-1, 64, 56, 56]          36,864\n",
      "           Conv2d-16           [-1, 64, 56, 56]          36,864\n",
      "        MaxPool2d-17           [-1, 64, 56, 56]               0\n",
      "        MaxPool2d-18           [-1, 64, 56, 56]               0\n",
      "      BatchNorm2d-19           [-1, 64, 56, 56]             128\n",
      "      BatchNorm2d-20           [-1, 64, 56, 56]             128\n",
      "           Conv2d-21           [-1, 64, 56, 56]          36,864\n",
      "             ReLU-22           [-1, 64, 56, 56]               0\n",
      "             ReLU-23           [-1, 64, 56, 56]               0\n",
      "           Conv2d-24           [-1, 64, 56, 56]          36,864\n",
      "           Conv2d-25           [-1, 64, 56, 56]          36,864\n",
      "           Conv2d-26           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-27           [-1, 64, 56, 56]             128\n",
      "      BatchNorm2d-28           [-1, 64, 56, 56]             128\n",
      "             ReLU-29           [-1, 64, 56, 56]               0\n",
      "             ReLU-30           [-1, 64, 56, 56]               0\n",
      "           Conv2d-31           [-1, 64, 56, 56]          36,864\n",
      "           Conv2d-32           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-33           [-1, 64, 56, 56]             128\n",
      "      BatchNorm2d-34           [-1, 64, 56, 56]             128\n",
      "      BatchNorm2d-35           [-1, 64, 56, 56]             128\n",
      "      BatchNorm2d-36           [-1, 64, 56, 56]             128\n",
      "             ReLU-37           [-1, 64, 56, 56]               0\n",
      "             ReLU-38           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-39           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-40           [-1, 64, 56, 56]               0\n",
      "           Conv2d-41           [-1, 64, 56, 56]          36,864\n",
      "           Conv2d-42           [-1, 64, 56, 56]          36,864\n",
      "             ReLU-43           [-1, 64, 56, 56]               0\n",
      "             ReLU-44           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-45           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-46           [-1, 64, 56, 56]               0\n",
      "           Conv2d-47           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-48           [-1, 64, 56, 56]             128\n",
      "           Conv2d-49           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-50           [-1, 64, 56, 56]             128\n",
      "             ReLU-51           [-1, 64, 56, 56]               0\n",
      "             ReLU-52           [-1, 64, 56, 56]               0\n",
      "           Conv2d-53           [-1, 64, 56, 56]          36,864\n",
      "           Conv2d-54           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-55           [-1, 64, 56, 56]             128\n",
      "      BatchNorm2d-56           [-1, 64, 56, 56]             128\n",
      "      BatchNorm2d-57           [-1, 64, 56, 56]             128\n",
      "      BatchNorm2d-58           [-1, 64, 56, 56]             128\n",
      "             ReLU-59           [-1, 64, 56, 56]               0\n",
      "             ReLU-60           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-61           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-62           [-1, 64, 56, 56]               0\n",
      "             ReLU-63           [-1, 64, 56, 56]               0\n",
      "             ReLU-64           [-1, 64, 56, 56]               0\n",
      "           Conv2d-65           [-1, 64, 56, 56]          36,864\n",
      "           Conv2d-66           [-1, 64, 56, 56]          36,864\n",
      "           Conv2d-67           [-1, 64, 56, 56]          36,864\n",
      "           Conv2d-68           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-69           [-1, 64, 56, 56]             128\n",
      "      BatchNorm2d-70           [-1, 64, 56, 56]             128\n",
      "             ReLU-71           [-1, 64, 56, 56]               0\n",
      "             ReLU-72           [-1, 64, 56, 56]               0\n",
      "           Conv2d-73           [-1, 64, 56, 56]          36,864\n",
      "           Conv2d-74           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-75           [-1, 64, 56, 56]             128\n",
      "      BatchNorm2d-76           [-1, 64, 56, 56]             128\n",
      "      BatchNorm2d-77           [-1, 64, 56, 56]             128\n",
      "             ReLU-78           [-1, 64, 56, 56]               0\n",
      "             ReLU-79           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-80           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-81           [-1, 64, 56, 56]               0\n",
      "      BatchNorm2d-82           [-1, 64, 56, 56]             128\n",
      "             ReLU-83           [-1, 64, 56, 56]               0\n",
      "             ReLU-84           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-85           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-86           [-1, 64, 56, 56]               0\n",
      "           Conv2d-87           [-1, 64, 56, 56]          36,864\n",
      "           Conv2d-88           [-1, 64, 56, 56]          36,864\n",
      "           Conv2d-89          [-1, 128, 28, 28]          73,728\n",
      "      BatchNorm2d-90           [-1, 64, 56, 56]             128\n",
      "           Conv2d-91          [-1, 128, 28, 28]          73,728\n",
      "      BatchNorm2d-92           [-1, 64, 56, 56]             128\n",
      "      BatchNorm2d-93          [-1, 128, 28, 28]             256\n",
      "             ReLU-94           [-1, 64, 56, 56]               0\n",
      "             ReLU-95           [-1, 64, 56, 56]               0\n",
      "      BatchNorm2d-96          [-1, 128, 28, 28]             256\n",
      "             ReLU-97          [-1, 128, 28, 28]               0\n",
      "             ReLU-98          [-1, 128, 28, 28]               0\n",
      "           Conv2d-99           [-1, 64, 56, 56]          36,864\n",
      "          Conv2d-100           [-1, 64, 56, 56]          36,864\n",
      "          Conv2d-101          [-1, 128, 28, 28]         147,456\n",
      "     BatchNorm2d-102           [-1, 64, 56, 56]             128\n",
      "          Conv2d-103          [-1, 128, 28, 28]         147,456\n",
      "     BatchNorm2d-104           [-1, 64, 56, 56]             128\n",
      "     BatchNorm2d-105          [-1, 128, 28, 28]             256\n",
      "     BatchNorm2d-106          [-1, 128, 28, 28]             256\n",
      "            ReLU-107           [-1, 64, 56, 56]               0\n",
      "            ReLU-108           [-1, 64, 56, 56]               0\n",
      "      BasicBlock-109           [-1, 64, 56, 56]               0\n",
      "      BasicBlock-110           [-1, 64, 56, 56]               0\n",
      "          Conv2d-111          [-1, 128, 28, 28]           8,192\n",
      "          Conv2d-112          [-1, 128, 28, 28]           8,192\n",
      "     BatchNorm2d-113          [-1, 128, 28, 28]             256\n",
      "     BatchNorm2d-114          [-1, 128, 28, 28]             256\n",
      "            ReLU-115          [-1, 128, 28, 28]               0\n",
      "            ReLU-116          [-1, 128, 28, 28]               0\n",
      "      BasicBlock-117          [-1, 128, 28, 28]               0\n",
      "      BasicBlock-118          [-1, 128, 28, 28]               0\n",
      "          Conv2d-119          [-1, 128, 28, 28]         147,456\n",
      "          Conv2d-120          [-1, 128, 28, 28]         147,456\n",
      "     BatchNorm2d-121          [-1, 128, 28, 28]             256\n",
      "     BatchNorm2d-122          [-1, 128, 28, 28]             256\n",
      "            ReLU-123          [-1, 128, 28, 28]               0\n",
      "            ReLU-124          [-1, 128, 28, 28]               0\n",
      "          Conv2d-125          [-1, 128, 28, 28]          73,728\n",
      "          Conv2d-126          [-1, 128, 28, 28]         147,456\n",
      "          Conv2d-127          [-1, 128, 28, 28]          73,728\n",
      "          Conv2d-128          [-1, 128, 28, 28]         147,456\n",
      "     BatchNorm2d-129          [-1, 128, 28, 28]             256\n",
      "     BatchNorm2d-130          [-1, 128, 28, 28]             256\n",
      "     BatchNorm2d-131          [-1, 128, 28, 28]             256\n",
      "     BatchNorm2d-132          [-1, 128, 28, 28]             256\n",
      "            ReLU-133          [-1, 128, 28, 28]               0\n",
      "            ReLU-134          [-1, 128, 28, 28]               0\n",
      "      BasicBlock-135          [-1, 128, 28, 28]               0\n",
      "      BasicBlock-136          [-1, 128, 28, 28]               0\n",
      "          Conv2d-137          [-1, 128, 28, 28]         147,456\n",
      "            ReLU-138          [-1, 128, 28, 28]               0\n",
      "            ReLU-139          [-1, 128, 28, 28]               0\n",
      "          Conv2d-140          [-1, 128, 28, 28]         147,456\n",
      "     BatchNorm2d-141          [-1, 128, 28, 28]             256\n",
      "     BatchNorm2d-142          [-1, 128, 28, 28]             256\n",
      "            ReLU-143          [-1, 128, 28, 28]               0\n",
      "            ReLU-144          [-1, 128, 28, 28]               0\n",
      "          Conv2d-145          [-1, 128, 28, 28]         147,456\n",
      "          Conv2d-146          [-1, 128, 28, 28]         147,456\n",
      "     BatchNorm2d-147          [-1, 128, 28, 28]             256\n",
      "          Conv2d-148          [-1, 128, 28, 28]         147,456\n",
      "          Conv2d-149          [-1, 128, 28, 28]         147,456\n",
      "     BatchNorm2d-150          [-1, 128, 28, 28]             256\n",
      "            ReLU-151          [-1, 128, 28, 28]               0\n",
      "            ReLU-152          [-1, 128, 28, 28]               0\n",
      "      BasicBlock-153          [-1, 128, 28, 28]               0\n",
      "      BasicBlock-154          [-1, 128, 28, 28]               0\n",
      "     BatchNorm2d-155          [-1, 128, 28, 28]             256\n",
      "     BatchNorm2d-156          [-1, 128, 28, 28]             256\n",
      "          Conv2d-157          [-1, 128, 28, 28]         147,456\n",
      "          Conv2d-158          [-1, 128, 28, 28]         147,456\n",
      "     BatchNorm2d-159          [-1, 128, 28, 28]             256\n",
      "     BatchNorm2d-160          [-1, 128, 28, 28]             256\n",
      "            ReLU-161          [-1, 128, 28, 28]               0\n",
      "            ReLU-162          [-1, 128, 28, 28]               0\n",
      "          Conv2d-163          [-1, 128, 28, 28]           8,192\n",
      "          Conv2d-164          [-1, 128, 28, 28]           8,192\n",
      "          Conv2d-165          [-1, 128, 28, 28]         147,456\n",
      "          Conv2d-166          [-1, 128, 28, 28]         147,456\n",
      "     BatchNorm2d-167          [-1, 128, 28, 28]             256\n",
      "     BatchNorm2d-168          [-1, 128, 28, 28]             256\n",
      "     BatchNorm2d-169          [-1, 128, 28, 28]             256\n",
      "     BatchNorm2d-170          [-1, 128, 28, 28]             256\n",
      "            ReLU-171          [-1, 128, 28, 28]               0\n",
      "            ReLU-172          [-1, 128, 28, 28]               0\n",
      "      BasicBlock-173          [-1, 128, 28, 28]               0\n",
      "      BasicBlock-174          [-1, 128, 28, 28]               0\n",
      "            ReLU-175          [-1, 128, 28, 28]               0\n",
      "            ReLU-176          [-1, 128, 28, 28]               0\n",
      "      BasicBlock-177          [-1, 128, 28, 28]               0\n",
      "      BasicBlock-178          [-1, 128, 28, 28]               0\n",
      "          Conv2d-179          [-1, 128, 28, 28]         147,456\n",
      "          Conv2d-180          [-1, 128, 28, 28]         147,456\n",
      "          Conv2d-181          [-1, 256, 14, 14]         294,912\n",
      "          Conv2d-182          [-1, 256, 14, 14]         294,912\n",
      "     BatchNorm2d-183          [-1, 128, 28, 28]             256\n",
      "     BatchNorm2d-184          [-1, 128, 28, 28]             256\n",
      "     BatchNorm2d-185          [-1, 256, 14, 14]             512\n",
      "     BatchNorm2d-186          [-1, 256, 14, 14]             512\n",
      "            ReLU-187          [-1, 128, 28, 28]               0\n",
      "            ReLU-188          [-1, 128, 28, 28]               0\n",
      "            ReLU-189          [-1, 256, 14, 14]               0\n",
      "            ReLU-190          [-1, 256, 14, 14]               0\n",
      "          Conv2d-191          [-1, 128, 28, 28]         147,456\n",
      "          Conv2d-192          [-1, 128, 28, 28]         147,456\n",
      "          Conv2d-193          [-1, 256, 14, 14]         589,824\n",
      "          Conv2d-194          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-195          [-1, 256, 14, 14]             512\n",
      "     BatchNorm2d-196          [-1, 128, 28, 28]             256\n",
      "     BatchNorm2d-197          [-1, 256, 14, 14]             512\n",
      "     BatchNorm2d-198          [-1, 128, 28, 28]             256\n",
      "          Conv2d-199          [-1, 256, 14, 14]          32,768\n",
      "          Conv2d-200          [-1, 256, 14, 14]          32,768\n",
      "            ReLU-201          [-1, 128, 28, 28]               0\n",
      "            ReLU-202          [-1, 128, 28, 28]               0\n",
      "      BasicBlock-203          [-1, 128, 28, 28]               0\n",
      "      BasicBlock-204          [-1, 128, 28, 28]               0\n",
      "     BatchNorm2d-205          [-1, 256, 14, 14]             512\n",
      "          Conv2d-206          [-1, 128, 28, 28]         147,456\n",
      "     BatchNorm2d-207          [-1, 256, 14, 14]             512\n",
      "          Conv2d-208          [-1, 128, 28, 28]         147,456\n",
      "            ReLU-209          [-1, 256, 14, 14]               0\n",
      "            ReLU-210          [-1, 256, 14, 14]               0\n",
      "      BasicBlock-211          [-1, 256, 14, 14]               0\n",
      "      BasicBlock-212          [-1, 256, 14, 14]               0\n",
      "          Conv2d-213          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-214          [-1, 128, 28, 28]             256\n",
      "          Conv2d-215          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-216          [-1, 128, 28, 28]             256\n",
      "     BatchNorm2d-217          [-1, 256, 14, 14]             512\n",
      "            ReLU-218          [-1, 128, 28, 28]               0\n",
      "            ReLU-219          [-1, 128, 28, 28]               0\n",
      "     BatchNorm2d-220          [-1, 256, 14, 14]             512\n",
      "            ReLU-221          [-1, 256, 14, 14]               0\n",
      "            ReLU-222          [-1, 256, 14, 14]               0\n",
      "          Conv2d-223          [-1, 128, 28, 28]         147,456\n",
      "          Conv2d-224          [-1, 128, 28, 28]         147,456\n",
      "          Conv2d-225          [-1, 256, 14, 14]         589,824\n",
      "          Conv2d-226          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-227          [-1, 128, 28, 28]             256\n",
      "     BatchNorm2d-228          [-1, 256, 14, 14]             512\n",
      "     BatchNorm2d-229          [-1, 128, 28, 28]             256\n",
      "     BatchNorm2d-230          [-1, 256, 14, 14]             512\n",
      "            ReLU-231          [-1, 256, 14, 14]               0\n",
      "            ReLU-232          [-1, 256, 14, 14]               0\n",
      "      BasicBlock-233          [-1, 256, 14, 14]               0\n",
      "      BasicBlock-234          [-1, 256, 14, 14]               0\n",
      "            ReLU-235          [-1, 128, 28, 28]               0\n",
      "            ReLU-236          [-1, 128, 28, 28]               0\n",
      "      BasicBlock-237          [-1, 128, 28, 28]               0\n",
      "      BasicBlock-238          [-1, 128, 28, 28]               0\n",
      "          Conv2d-239          [-1, 256, 14, 14]         589,824\n",
      "          Conv2d-240          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-241          [-1, 256, 14, 14]             512\n",
      "     BatchNorm2d-242          [-1, 256, 14, 14]             512\n",
      "            ReLU-243          [-1, 256, 14, 14]               0\n",
      "            ReLU-244          [-1, 256, 14, 14]               0\n",
      "          Conv2d-245          [-1, 256, 14, 14]         589,824\n",
      "          Conv2d-246          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-247          [-1, 256, 14, 14]             512\n",
      "          Conv2d-248          [-1, 128, 28, 28]         147,456\n",
      "     BatchNorm2d-249          [-1, 256, 14, 14]             512\n",
      "          Conv2d-250          [-1, 128, 28, 28]         147,456\n",
      "            ReLU-251          [-1, 256, 14, 14]               0\n",
      "            ReLU-252          [-1, 256, 14, 14]               0\n",
      "      BasicBlock-253          [-1, 256, 14, 14]               0\n",
      "      BasicBlock-254          [-1, 256, 14, 14]               0\n",
      "          Conv2d-255          [-1, 256, 14, 14]         589,824\n",
      "          Conv2d-256          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-257          [-1, 128, 28, 28]             256\n",
      "     BatchNorm2d-258          [-1, 128, 28, 28]             256\n",
      "     BatchNorm2d-259          [-1, 256, 14, 14]             512\n",
      "     BatchNorm2d-260          [-1, 256, 14, 14]             512\n",
      "            ReLU-261          [-1, 128, 28, 28]               0\n",
      "            ReLU-262          [-1, 128, 28, 28]               0\n",
      "            ReLU-263          [-1, 256, 14, 14]               0\n",
      "            ReLU-264          [-1, 256, 14, 14]               0\n",
      "          Conv2d-265          [-1, 256, 14, 14]         589,824\n",
      "          Conv2d-266          [-1, 128, 28, 28]         147,456\n",
      "          Conv2d-267          [-1, 256, 14, 14]         589,824\n",
      "          Conv2d-268          [-1, 128, 28, 28]         147,456\n",
      "     BatchNorm2d-269          [-1, 256, 14, 14]             512\n",
      "     BatchNorm2d-270          [-1, 256, 14, 14]             512\n",
      "     BatchNorm2d-271          [-1, 128, 28, 28]             256\n",
      "            ReLU-272          [-1, 256, 14, 14]               0\n",
      "            ReLU-273          [-1, 256, 14, 14]               0\n",
      "      BasicBlock-274          [-1, 256, 14, 14]               0\n",
      "      BasicBlock-275          [-1, 256, 14, 14]               0\n",
      "     BatchNorm2d-276          [-1, 128, 28, 28]             256\n",
      "            ReLU-277          [-1, 128, 28, 28]               0\n",
      "            ReLU-278          [-1, 128, 28, 28]               0\n",
      "      BasicBlock-279          [-1, 128, 28, 28]               0\n",
      "      BasicBlock-280          [-1, 128, 28, 28]               0\n",
      "          Conv2d-281          [-1, 256, 14, 14]         589,824\n",
      "          Conv2d-282          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-283          [-1, 256, 14, 14]             512\n",
      "     BatchNorm2d-284          [-1, 256, 14, 14]             512\n",
      "            ReLU-285          [-1, 256, 14, 14]               0\n",
      "            ReLU-286          [-1, 256, 14, 14]               0\n",
      "          Conv2d-287          [-1, 256, 14, 14]         589,824\n",
      "          Conv2d-288          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-289          [-1, 256, 14, 14]             512\n",
      "     BatchNorm2d-290          [-1, 256, 14, 14]             512\n",
      "            ReLU-291          [-1, 256, 14, 14]               0\n",
      "            ReLU-292          [-1, 256, 14, 14]               0\n",
      "      BasicBlock-293          [-1, 256, 14, 14]               0\n",
      "      BasicBlock-294          [-1, 256, 14, 14]               0\n",
      "          Conv2d-295          [-1, 256, 14, 14]         589,824\n",
      "          Conv2d-296          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-297          [-1, 256, 14, 14]             512\n",
      "     BatchNorm2d-298          [-1, 256, 14, 14]             512\n",
      "            ReLU-299          [-1, 256, 14, 14]               0\n",
      "            ReLU-300          [-1, 256, 14, 14]               0\n",
      "          Conv2d-301          [-1, 256, 14, 14]         294,912\n",
      "          Conv2d-302          [-1, 256, 14, 14]         589,824\n",
      "          Conv2d-303          [-1, 256, 14, 14]         294,912\n",
      "          Conv2d-304          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-305          [-1, 256, 14, 14]             512\n",
      "     BatchNorm2d-306          [-1, 256, 14, 14]             512\n",
      "     BatchNorm2d-307          [-1, 256, 14, 14]             512\n",
      "     BatchNorm2d-308          [-1, 256, 14, 14]             512\n",
      "            ReLU-309          [-1, 256, 14, 14]               0\n",
      "            ReLU-310          [-1, 256, 14, 14]               0\n",
      "      BasicBlock-311          [-1, 256, 14, 14]               0\n",
      "      BasicBlock-312          [-1, 256, 14, 14]               0\n",
      "            ReLU-313          [-1, 256, 14, 14]               0\n",
      "            ReLU-314          [-1, 256, 14, 14]               0\n",
      "          Conv2d-315            [-1, 512, 7, 7]       1,179,648\n",
      "          Conv2d-316            [-1, 512, 7, 7]       1,179,648\n",
      "     BatchNorm2d-317            [-1, 512, 7, 7]           1,024\n",
      "     BatchNorm2d-318            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-319            [-1, 512, 7, 7]               0\n",
      "            ReLU-320            [-1, 512, 7, 7]               0\n",
      "          Conv2d-321          [-1, 256, 14, 14]         589,824\n",
      "          Conv2d-322          [-1, 256, 14, 14]         589,824\n",
      "          Conv2d-323            [-1, 512, 7, 7]       2,359,296\n",
      "          Conv2d-324            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-325          [-1, 256, 14, 14]             512\n",
      "     BatchNorm2d-326            [-1, 512, 7, 7]           1,024\n",
      "     BatchNorm2d-327          [-1, 256, 14, 14]             512\n",
      "     BatchNorm2d-328            [-1, 512, 7, 7]           1,024\n",
      "          Conv2d-329          [-1, 256, 14, 14]          32,768\n",
      "          Conv2d-330          [-1, 256, 14, 14]          32,768\n",
      "          Conv2d-331            [-1, 512, 7, 7]         131,072\n",
      "          Conv2d-332            [-1, 512, 7, 7]         131,072\n",
      "     BatchNorm2d-333          [-1, 256, 14, 14]             512\n",
      "     BatchNorm2d-334          [-1, 256, 14, 14]             512\n",
      "     BatchNorm2d-335            [-1, 512, 7, 7]           1,024\n",
      "     BatchNorm2d-336            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-337            [-1, 512, 7, 7]               0\n",
      "            ReLU-338            [-1, 512, 7, 7]               0\n",
      "      BasicBlock-339            [-1, 512, 7, 7]               0\n",
      "      BasicBlock-340            [-1, 512, 7, 7]               0\n",
      "            ReLU-341          [-1, 256, 14, 14]               0\n",
      "            ReLU-342          [-1, 256, 14, 14]               0\n",
      "      BasicBlock-343          [-1, 256, 14, 14]               0\n",
      "      BasicBlock-344          [-1, 256, 14, 14]               0\n",
      "          Conv2d-345            [-1, 512, 7, 7]       2,359,296\n",
      "          Conv2d-346            [-1, 512, 7, 7]       2,359,296\n",
      "          Conv2d-347          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-348            [-1, 512, 7, 7]           1,024\n",
      "          Conv2d-349          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-350            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-351            [-1, 512, 7, 7]               0\n",
      "            ReLU-352            [-1, 512, 7, 7]               0\n",
      "     BatchNorm2d-353          [-1, 256, 14, 14]             512\n",
      "          Conv2d-354            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-355          [-1, 256, 14, 14]             512\n",
      "          Conv2d-356            [-1, 512, 7, 7]       2,359,296\n",
      "            ReLU-357          [-1, 256, 14, 14]               0\n",
      "            ReLU-358          [-1, 256, 14, 14]               0\n",
      "     BatchNorm2d-359            [-1, 512, 7, 7]           1,024\n",
      "     BatchNorm2d-360            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-361            [-1, 512, 7, 7]               0\n",
      "            ReLU-362            [-1, 512, 7, 7]               0\n",
      "      BasicBlock-363            [-1, 512, 7, 7]               0\n",
      "      BasicBlock-364            [-1, 512, 7, 7]               0\n",
      "          Conv2d-365          [-1, 256, 14, 14]         589,824\n",
      "          Conv2d-366          [-1, 256, 14, 14]         589,824\n",
      "          Conv2d-367            [-1, 512, 7, 7]       2,359,296\n",
      "          Conv2d-368            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-369          [-1, 256, 14, 14]             512\n",
      "     BatchNorm2d-370            [-1, 512, 7, 7]           1,024\n",
      "     BatchNorm2d-371          [-1, 256, 14, 14]             512\n",
      "     BatchNorm2d-372            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-373            [-1, 512, 7, 7]               0\n",
      "            ReLU-374            [-1, 512, 7, 7]               0\n",
      "            ReLU-375          [-1, 256, 14, 14]               0\n",
      "            ReLU-376          [-1, 256, 14, 14]               0\n",
      "      BasicBlock-377          [-1, 256, 14, 14]               0\n",
      "      BasicBlock-378          [-1, 256, 14, 14]               0\n",
      "          Conv2d-379            [-1, 512, 7, 7]       2,359,296\n",
      "          Conv2d-380            [-1, 512, 7, 7]       2,359,296\n",
      "          Conv2d-381          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-382            [-1, 512, 7, 7]           1,024\n",
      "          Conv2d-383          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-384            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-385            [-1, 512, 7, 7]               0\n",
      "            ReLU-386            [-1, 512, 7, 7]               0\n",
      "      BasicBlock-387            [-1, 512, 7, 7]               0\n",
      "      BasicBlock-388            [-1, 512, 7, 7]               0\n",
      "     BatchNorm2d-389          [-1, 256, 14, 14]             512\n",
      "     BatchNorm2d-390          [-1, 256, 14, 14]             512\n",
      "            ReLU-391          [-1, 256, 14, 14]               0\n",
      "            ReLU-392          [-1, 256, 14, 14]               0\n",
      "          Conv2d-393         [-1, 64, 112, 112]           9,408\n",
      "          Conv2d-394         [-1, 64, 112, 112]           9,408\n",
      "          Conv2d-395          [-1, 256, 14, 14]         589,824\n",
      "          Conv2d-396          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-397         [-1, 64, 112, 112]             128\n",
      "     BatchNorm2d-398         [-1, 64, 112, 112]             128\n",
      "     BatchNorm2d-399          [-1, 256, 14, 14]             512\n",
      "            ReLU-400         [-1, 64, 112, 112]               0\n",
      "            ReLU-401         [-1, 64, 112, 112]               0\n",
      "       MaxPool2d-402           [-1, 64, 56, 56]               0\n",
      "       MaxPool2d-403           [-1, 64, 56, 56]               0\n",
      "     BatchNorm2d-404          [-1, 256, 14, 14]             512\n",
      "            ReLU-405          [-1, 256, 14, 14]               0\n",
      "            ReLU-406          [-1, 256, 14, 14]               0\n",
      "      BasicBlock-407          [-1, 256, 14, 14]               0\n",
      "      BasicBlock-408          [-1, 256, 14, 14]               0\n",
      "          Conv2d-409           [-1, 64, 56, 56]          36,864\n",
      "          Conv2d-410           [-1, 64, 56, 56]          36,864\n",
      "     BatchNorm2d-411           [-1, 64, 56, 56]             128\n",
      "     BatchNorm2d-412           [-1, 64, 56, 56]             128\n",
      "          Conv2d-413          [-1, 256, 14, 14]         589,824\n",
      "          Conv2d-414          [-1, 256, 14, 14]         589,824\n",
      "            ReLU-415           [-1, 64, 56, 56]               0\n",
      "            ReLU-416           [-1, 64, 56, 56]               0\n",
      "     BatchNorm2d-417          [-1, 256, 14, 14]             512\n",
      "     BatchNorm2d-418          [-1, 256, 14, 14]             512\n",
      "            ReLU-419          [-1, 256, 14, 14]               0\n",
      "            ReLU-420          [-1, 256, 14, 14]               0\n",
      "          Conv2d-421           [-1, 64, 56, 56]          36,864\n",
      "          Conv2d-422           [-1, 64, 56, 56]          36,864\n",
      "          Conv2d-423          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-424           [-1, 64, 56, 56]             128\n",
      "          Conv2d-425          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-426           [-1, 64, 56, 56]             128\n",
      "            ReLU-427           [-1, 64, 56, 56]               0\n",
      "            ReLU-428           [-1, 64, 56, 56]               0\n",
      "      BasicBlock-429           [-1, 64, 56, 56]               0\n",
      "      BasicBlock-430           [-1, 64, 56, 56]               0\n",
      "     BatchNorm2d-431          [-1, 256, 14, 14]             512\n",
      "     BatchNorm2d-432          [-1, 256, 14, 14]             512\n",
      "            ReLU-433          [-1, 256, 14, 14]               0\n",
      "            ReLU-434          [-1, 256, 14, 14]               0\n",
      "      BasicBlock-435          [-1, 256, 14, 14]               0\n",
      "      BasicBlock-436          [-1, 256, 14, 14]               0\n",
      "          Conv2d-437           [-1, 64, 56, 56]          36,864\n",
      "          Conv2d-438          [-1, 256, 14, 14]         589,824\n",
      "          Conv2d-439           [-1, 64, 56, 56]          36,864\n",
      "          Conv2d-440          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-441           [-1, 64, 56, 56]             128\n",
      "     BatchNorm2d-442           [-1, 64, 56, 56]             128\n",
      "     BatchNorm2d-443          [-1, 256, 14, 14]             512\n",
      "     BatchNorm2d-444          [-1, 256, 14, 14]             512\n",
      "            ReLU-445           [-1, 64, 56, 56]               0\n",
      "            ReLU-446           [-1, 64, 56, 56]               0\n",
      "            ReLU-447          [-1, 256, 14, 14]               0\n",
      "            ReLU-448          [-1, 256, 14, 14]               0\n",
      "          Conv2d-449          [-1, 256, 14, 14]         589,824\n",
      "          Conv2d-450          [-1, 256, 14, 14]         589,824\n",
      "          Conv2d-451           [-1, 64, 56, 56]          36,864\n",
      "          Conv2d-452           [-1, 64, 56, 56]          36,864\n",
      "     BatchNorm2d-453          [-1, 256, 14, 14]             512\n",
      "     BatchNorm2d-454           [-1, 64, 56, 56]             128\n",
      "     BatchNorm2d-455           [-1, 64, 56, 56]             128\n",
      "     BatchNorm2d-456          [-1, 256, 14, 14]             512\n",
      "            ReLU-457           [-1, 64, 56, 56]               0\n",
      "            ReLU-458           [-1, 64, 56, 56]               0\n",
      "      BasicBlock-459           [-1, 64, 56, 56]               0\n",
      "      BasicBlock-460           [-1, 64, 56, 56]               0\n",
      "            ReLU-461          [-1, 256, 14, 14]               0\n",
      "            ReLU-462          [-1, 256, 14, 14]               0\n",
      "      BasicBlock-463          [-1, 256, 14, 14]               0\n",
      "      BasicBlock-464          [-1, 256, 14, 14]               0\n",
      "          Conv2d-465          [-1, 256, 14, 14]         589,824\n",
      "          Conv2d-466          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-467          [-1, 256, 14, 14]             512\n",
      "     BatchNorm2d-468          [-1, 256, 14, 14]             512\n",
      "            ReLU-469          [-1, 256, 14, 14]               0\n",
      "            ReLU-470          [-1, 256, 14, 14]               0\n",
      "          Conv2d-471          [-1, 128, 28, 28]          73,728\n",
      "          Conv2d-472          [-1, 128, 28, 28]          73,728\n",
      "     BatchNorm2d-473          [-1, 128, 28, 28]             256\n",
      "     BatchNorm2d-474          [-1, 128, 28, 28]             256\n",
      "            ReLU-475          [-1, 128, 28, 28]               0\n",
      "            ReLU-476          [-1, 128, 28, 28]               0\n",
      "          Conv2d-477          [-1, 256, 14, 14]         589,824\n",
      "          Conv2d-478          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-479          [-1, 256, 14, 14]             512\n",
      "     BatchNorm2d-480          [-1, 256, 14, 14]             512\n",
      "            ReLU-481          [-1, 256, 14, 14]               0\n",
      "            ReLU-482          [-1, 256, 14, 14]               0\n",
      "      BasicBlock-483          [-1, 256, 14, 14]               0\n",
      "      BasicBlock-484          [-1, 256, 14, 14]               0\n",
      "          Conv2d-485          [-1, 128, 28, 28]         147,456\n",
      "          Conv2d-486          [-1, 128, 28, 28]         147,456\n",
      "     BatchNorm2d-487          [-1, 128, 28, 28]             256\n",
      "     BatchNorm2d-488          [-1, 128, 28, 28]             256\n",
      "          Conv2d-489          [-1, 128, 28, 28]           8,192\n",
      "          Conv2d-490          [-1, 128, 28, 28]           8,192\n",
      "     BatchNorm2d-491          [-1, 128, 28, 28]             256\n",
      "     BatchNorm2d-492          [-1, 128, 28, 28]             256\n",
      "            ReLU-493          [-1, 128, 28, 28]               0\n",
      "            ReLU-494          [-1, 128, 28, 28]               0\n",
      "      BasicBlock-495          [-1, 128, 28, 28]               0\n",
      "      BasicBlock-496          [-1, 128, 28, 28]               0\n",
      "          Conv2d-497          [-1, 128, 28, 28]         147,456\n",
      "          Conv2d-498          [-1, 128, 28, 28]         147,456\n",
      "          Conv2d-499            [-1, 512, 7, 7]       1,179,648\n",
      "     BatchNorm2d-500          [-1, 128, 28, 28]             256\n",
      "          Conv2d-501            [-1, 512, 7, 7]       1,179,648\n",
      "     BatchNorm2d-502          [-1, 128, 28, 28]             256\n",
      "            ReLU-503          [-1, 128, 28, 28]               0\n",
      "            ReLU-504          [-1, 128, 28, 28]               0\n",
      "     BatchNorm2d-505            [-1, 512, 7, 7]           1,024\n",
      "     BatchNorm2d-506            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-507            [-1, 512, 7, 7]               0\n",
      "            ReLU-508            [-1, 512, 7, 7]               0\n",
      "          Conv2d-509          [-1, 128, 28, 28]         147,456\n",
      "          Conv2d-510          [-1, 128, 28, 28]         147,456\n",
      "     BatchNorm2d-511          [-1, 128, 28, 28]             256\n",
      "     BatchNorm2d-512          [-1, 128, 28, 28]             256\n",
      "            ReLU-513          [-1, 128, 28, 28]               0\n",
      "            ReLU-514          [-1, 128, 28, 28]               0\n",
      "      BasicBlock-515          [-1, 128, 28, 28]               0\n",
      "      BasicBlock-516          [-1, 128, 28, 28]               0\n",
      "          Conv2d-517            [-1, 512, 7, 7]       2,359,296\n",
      "          Conv2d-518          [-1, 256, 14, 14]         294,912\n",
      "          Conv2d-519            [-1, 512, 7, 7]       2,359,296\n",
      "          Conv2d-520          [-1, 256, 14, 14]         294,912\n",
      "     BatchNorm2d-521          [-1, 256, 14, 14]             512\n",
      "     BatchNorm2d-522          [-1, 256, 14, 14]             512\n",
      "     BatchNorm2d-523            [-1, 512, 7, 7]           1,024\n",
      "     BatchNorm2d-524            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-525          [-1, 256, 14, 14]               0\n",
      "            ReLU-526          [-1, 256, 14, 14]               0\n",
      "          Conv2d-527            [-1, 512, 7, 7]         131,072\n",
      "          Conv2d-528            [-1, 512, 7, 7]         131,072\n",
      "          Conv2d-529          [-1, 256, 14, 14]         589,824\n",
      "          Conv2d-530          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-531            [-1, 512, 7, 7]           1,024\n",
      "     BatchNorm2d-532            [-1, 512, 7, 7]           1,024\n",
      "     BatchNorm2d-533          [-1, 256, 14, 14]             512\n",
      "     BatchNorm2d-534          [-1, 256, 14, 14]             512\n",
      "            ReLU-535            [-1, 512, 7, 7]               0\n",
      "            ReLU-536            [-1, 512, 7, 7]               0\n",
      "      BasicBlock-537            [-1, 512, 7, 7]               0\n",
      "      BasicBlock-538            [-1, 512, 7, 7]               0\n",
      "          Conv2d-539          [-1, 256, 14, 14]          32,768\n",
      "          Conv2d-540            [-1, 512, 7, 7]       2,359,296\n",
      "          Conv2d-541          [-1, 256, 14, 14]          32,768\n",
      "          Conv2d-542            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-543          [-1, 256, 14, 14]             512\n",
      "     BatchNorm2d-544            [-1, 512, 7, 7]           1,024\n",
      "     BatchNorm2d-545          [-1, 256, 14, 14]             512\n",
      "     BatchNorm2d-546            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-547          [-1, 256, 14, 14]               0\n",
      "            ReLU-548          [-1, 256, 14, 14]               0\n",
      "      BasicBlock-549          [-1, 256, 14, 14]               0\n",
      "      BasicBlock-550          [-1, 256, 14, 14]               0\n",
      "            ReLU-551            [-1, 512, 7, 7]               0\n",
      "            ReLU-552            [-1, 512, 7, 7]               0\n",
      "          Conv2d-553            [-1, 512, 7, 7]       2,359,296\n",
      "          Conv2d-554            [-1, 512, 7, 7]       2,359,296\n",
      "          Conv2d-555          [-1, 256, 14, 14]         589,824\n",
      "          Conv2d-556          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-557            [-1, 512, 7, 7]           1,024\n",
      "     BatchNorm2d-558          [-1, 256, 14, 14]             512\n",
      "     BatchNorm2d-559            [-1, 512, 7, 7]           1,024\n",
      "     BatchNorm2d-560          [-1, 256, 14, 14]             512\n",
      "            ReLU-561          [-1, 256, 14, 14]               0\n",
      "            ReLU-562          [-1, 256, 14, 14]               0\n",
      "            ReLU-563            [-1, 512, 7, 7]               0\n",
      "            ReLU-564            [-1, 512, 7, 7]               0\n",
      "      BasicBlock-565            [-1, 512, 7, 7]               0\n",
      "      BasicBlock-566            [-1, 512, 7, 7]               0\n",
      "          Conv2d-567            [-1, 512, 7, 7]       2,359,296\n",
      "          Conv2d-568            [-1, 512, 7, 7]       2,359,296\n",
      "          Conv2d-569          [-1, 256, 14, 14]         589,824\n",
      "          Conv2d-570          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-571            [-1, 512, 7, 7]           1,024\n",
      "     BatchNorm2d-572          [-1, 256, 14, 14]             512\n",
      "     BatchNorm2d-573            [-1, 512, 7, 7]           1,024\n",
      "     BatchNorm2d-574          [-1, 256, 14, 14]             512\n",
      "            ReLU-575            [-1, 512, 7, 7]               0\n",
      "            ReLU-576            [-1, 512, 7, 7]               0\n",
      "            ReLU-577          [-1, 256, 14, 14]               0\n",
      "            ReLU-578          [-1, 256, 14, 14]               0\n",
      "      BasicBlock-579          [-1, 256, 14, 14]               0\n",
      "      BasicBlock-580          [-1, 256, 14, 14]               0\n",
      "          Conv2d-581            [-1, 512, 7, 7]       2,359,296\n",
      "          Conv2d-582            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-583            [-1, 512, 7, 7]           1,024\n",
      "     BatchNorm2d-584            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-585            [-1, 512, 7, 7]               0\n",
      "            ReLU-586            [-1, 512, 7, 7]               0\n",
      "      BasicBlock-587            [-1, 512, 7, 7]               0\n",
      "      BasicBlock-588            [-1, 512, 7, 7]               0\n",
      "          Conv2d-589            [-1, 512, 7, 7]       1,179,648\n",
      "          Conv2d-590            [-1, 512, 7, 7]       1,179,648\n",
      "          Conv2d-591         [-1, 64, 112, 112]           9,408\n",
      "     BatchNorm2d-592            [-1, 512, 7, 7]           1,024\n",
      "          Conv2d-593         [-1, 64, 112, 112]           9,408\n",
      "     BatchNorm2d-594            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-595            [-1, 512, 7, 7]               0\n",
      "            ReLU-596            [-1, 512, 7, 7]               0\n",
      "     BatchNorm2d-597         [-1, 64, 112, 112]             128\n",
      "     BatchNorm2d-598         [-1, 64, 112, 112]             128\n",
      "            ReLU-599         [-1, 64, 112, 112]               0\n",
      "            ReLU-600         [-1, 64, 112, 112]               0\n",
      "       MaxPool2d-601           [-1, 64, 56, 56]               0\n",
      "       MaxPool2d-602           [-1, 64, 56, 56]               0\n",
      "          Conv2d-603            [-1, 512, 7, 7]       2,359,296\n",
      "          Conv2d-604            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-605            [-1, 512, 7, 7]           1,024\n",
      "          Conv2d-606           [-1, 64, 56, 56]          36,864\n",
      "     BatchNorm2d-607            [-1, 512, 7, 7]           1,024\n",
      "          Conv2d-608           [-1, 64, 56, 56]          36,864\n",
      "     BatchNorm2d-609           [-1, 64, 56, 56]             128\n",
      "     BatchNorm2d-610           [-1, 64, 56, 56]             128\n",
      "          Conv2d-611            [-1, 512, 7, 7]         131,072\n",
      "          Conv2d-612            [-1, 512, 7, 7]         131,072\n",
      "            ReLU-613           [-1, 64, 56, 56]               0\n",
      "            ReLU-614           [-1, 64, 56, 56]               0\n",
      "     BatchNorm2d-615            [-1, 512, 7, 7]           1,024\n",
      "     BatchNorm2d-616            [-1, 512, 7, 7]           1,024\n",
      "          Conv2d-617           [-1, 64, 56, 56]          36,864\n",
      "          Conv2d-618           [-1, 64, 56, 56]          36,864\n",
      "            ReLU-619            [-1, 512, 7, 7]               0\n",
      "            ReLU-620            [-1, 512, 7, 7]               0\n",
      "      BasicBlock-621            [-1, 512, 7, 7]               0\n",
      "      BasicBlock-622            [-1, 512, 7, 7]               0\n",
      "          Conv2d-623            [-1, 512, 7, 7]       2,359,296\n",
      "          Conv2d-624            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-625           [-1, 64, 56, 56]             128\n",
      "     BatchNorm2d-626           [-1, 64, 56, 56]             128\n",
      "     BatchNorm2d-627            [-1, 512, 7, 7]           1,024\n",
      "     BatchNorm2d-628            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-629           [-1, 64, 56, 56]               0\n",
      "            ReLU-630           [-1, 64, 56, 56]               0\n",
      "      BasicBlock-631           [-1, 64, 56, 56]               0\n",
      "      BasicBlock-632           [-1, 64, 56, 56]               0\n",
      "            ReLU-633            [-1, 512, 7, 7]               0\n",
      "            ReLU-634            [-1, 512, 7, 7]               0\n",
      "          Conv2d-635           [-1, 64, 56, 56]          36,864\n",
      "          Conv2d-636           [-1, 64, 56, 56]          36,864\n",
      "          Conv2d-637            [-1, 512, 7, 7]       2,359,296\n",
      "          Conv2d-638            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-639            [-1, 512, 7, 7]           1,024\n",
      "     BatchNorm2d-640            [-1, 512, 7, 7]           1,024\n",
      "     BatchNorm2d-641           [-1, 64, 56, 56]             128\n",
      "            ReLU-642            [-1, 512, 7, 7]               0\n",
      "            ReLU-643            [-1, 512, 7, 7]               0\n",
      "      BasicBlock-644            [-1, 512, 7, 7]               0\n",
      "      BasicBlock-645            [-1, 512, 7, 7]               0\n",
      "     BatchNorm2d-646           [-1, 64, 56, 56]             128\n",
      "            ReLU-647           [-1, 64, 56, 56]               0\n",
      "            ReLU-648           [-1, 64, 56, 56]               0\n",
      "          Conv2d-649           [-1, 64, 56, 56]          36,864\n",
      "          Conv2d-650           [-1, 64, 56, 56]          36,864\n",
      "     BatchNorm2d-651           [-1, 64, 56, 56]             128\n",
      "     BatchNorm2d-652           [-1, 64, 56, 56]             128\n",
      "            ReLU-653           [-1, 64, 56, 56]               0\n",
      "            ReLU-654           [-1, 64, 56, 56]               0\n",
      "      BasicBlock-655           [-1, 64, 56, 56]               0\n",
      "      BasicBlock-656           [-1, 64, 56, 56]               0\n",
      "          Linear-657              [-1, 49, 200]         102,600\n",
      "       LeakyReLU-658              [-1, 49, 200]               0\n",
      "         Dropout-659              [-1, 49, 200]               0\n",
      "          Conv2d-660          [-1, 128, 28, 28]          73,728\n",
      "          Linear-661              [-1, 49, 120]          24,120\n",
      "          Conv2d-662          [-1, 128, 28, 28]          73,728\n",
      "       LeakyReLU-663              [-1, 49, 120]               0\n",
      "     BatchNorm2d-664          [-1, 128, 28, 28]             256\n",
      "         Dropout-665              [-1, 49, 120]               0\n",
      "     BatchNorm2d-666          [-1, 128, 28, 28]             256\n",
      "          Linear-667               [-1, 49, 80]           9,680\n",
      "            ReLU-668          [-1, 128, 28, 28]               0\n",
      "            ReLU-669          [-1, 128, 28, 28]               0\n",
      "       LeakyReLU-670               [-1, 49, 80]               0\n",
      "         Dropout-671               [-1, 49, 80]               0\n",
      "          Linear-672               [-1, 49, 50]           4,050\n",
      "          Linear-673              [-1, 49, 200]         102,600\n",
      "       LeakyReLU-674              [-1, 49, 200]               0\n",
      "          Conv2d-675          [-1, 128, 28, 28]         147,456\n",
      "         Dropout-676              [-1, 49, 200]               0\n",
      "          Conv2d-677          [-1, 128, 28, 28]         147,456\n",
      "          Linear-678              [-1, 49, 120]          24,120\n",
      "     BatchNorm2d-679          [-1, 128, 28, 28]             256\n",
      "       LeakyReLU-680              [-1, 49, 120]               0\n",
      "     BatchNorm2d-681          [-1, 128, 28, 28]             256\n",
      "         Dropout-682              [-1, 49, 120]               0\n",
      "          Linear-683               [-1, 49, 80]           9,680\n",
      "       LeakyReLU-684               [-1, 49, 80]               0\n",
      "          Conv2d-685          [-1, 128, 28, 28]           8,192\n",
      "         Dropout-686               [-1, 49, 80]               0\n",
      "          Conv2d-687          [-1, 128, 28, 28]           8,192\n",
      "          Linear-688               [-1, 49, 50]           4,050\n",
      "     BatchNorm2d-689          [-1, 128, 28, 28]             256\n",
      "     BatchNorm2d-690          [-1, 128, 28, 28]             256\n",
      "            ReLU-691          [-1, 128, 28, 28]               0\n",
      "            ReLU-692          [-1, 128, 28, 28]               0\n",
      "      BasicBlock-693          [-1, 128, 28, 28]               0\n",
      "      BasicBlock-694          [-1, 128, 28, 28]               0\n",
      "          Linear-695                [-1, 12000]     307,212,000\n",
      "       LeakyReLU-696                [-1, 12000]               0\n",
      "         Dropout-697                [-1, 12000]               0\n",
      "          Linear-698                 [-1, 6000]      72,006,000\n",
      "          Linear-699                [-1, 12000]     307,212,000\n",
      "       LeakyReLU-700                [-1, 12000]               0\n",
      "          Conv2d-701          [-1, 128, 28, 28]         147,456\n",
      "         Dropout-702                [-1, 12000]               0\n",
      "          Conv2d-703          [-1, 128, 28, 28]         147,456\n",
      "          Linear-704                 [-1, 6000]      72,006,000\n",
      "     BatchNorm2d-705          [-1, 128, 28, 28]             256\n",
      "     BatchNorm2d-706          [-1, 128, 28, 28]             256\n",
      "          Linear-707                  [-1, 400]       2,400,400\n",
      "            ReLU-708          [-1, 128, 28, 28]               0\n",
      "            ReLU-709          [-1, 128, 28, 28]               0\n",
      "       LeakyReLU-710                  [-1, 400]               0\n",
      "         Dropout-711                  [-1, 400]               0\n",
      "          Linear-712                   [-1, 80]          32,080\n",
      "       LeakyReLU-713                   [-1, 80]               0\n",
      "          Conv2d-714          [-1, 128, 28, 28]         147,456\n",
      "         Dropout-715                   [-1, 80]               0\n",
      "          Conv2d-716          [-1, 128, 28, 28]         147,456\n",
      "          Linear-717                   [-1, 30]           2,430\n",
      "     BatchNorm2d-718          [-1, 128, 28, 28]             256\n",
      "       LeakyReLU-719                   [-1, 30]               0\n",
      "         Dropout-720                   [-1, 30]               0\n",
      "     BatchNorm2d-721          [-1, 128, 28, 28]             256\n",
      "          Linear-722                   [-1, 10]             310\n",
      "            ReLU-723          [-1, 128, 28, 28]               0\n",
      "            ReLU-724          [-1, 128, 28, 28]               0\n",
      "      BasicBlock-725          [-1, 128, 28, 28]               0\n",
      "      BasicBlock-726          [-1, 128, 28, 28]               0\n",
      "       LeakyReLU-727                   [-1, 10]               0\n",
      "         Dropout-728                   [-1, 10]               0\n",
      "          Linear-729                    [-1, 1]              11\n",
      "          Linear-730                  [-1, 400]       2,400,400\n",
      "       LeakyReLU-731                  [-1, 400]               0\n",
      "         Dropout-732                  [-1, 400]               0\n",
      "          Linear-733                   [-1, 80]          32,080\n",
      "          Conv2d-734          [-1, 256, 14, 14]         294,912\n",
      "          Conv2d-735          [-1, 256, 14, 14]         294,912\n",
      "       LeakyReLU-736                   [-1, 80]               0\n",
      "         Dropout-737                   [-1, 80]               0\n",
      "     BatchNorm2d-738          [-1, 256, 14, 14]             512\n",
      "     BatchNorm2d-739          [-1, 256, 14, 14]             512\n",
      "          Linear-740                   [-1, 30]           2,430\n",
      "            ReLU-741          [-1, 256, 14, 14]               0\n",
      "            ReLU-742          [-1, 256, 14, 14]               0\n",
      "       LeakyReLU-743                   [-1, 30]               0\n",
      "         Dropout-744                   [-1, 30]               0\n",
      "          Linear-745                   [-1, 10]             310\n",
      "       LeakyReLU-746                   [-1, 10]               0\n",
      "         Dropout-747                   [-1, 10]               0\n",
      "          Linear-748                    [-1, 1]              11\n",
      "          Conv2d-749          [-1, 256, 14, 14]         589,824\n",
      "          Conv2d-750          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-751          [-1, 256, 14, 14]             512\n",
      "     BatchNorm2d-752          [-1, 256, 14, 14]             512\n",
      "          Conv2d-753          [-1, 256, 14, 14]          32,768\n",
      "          Conv2d-754          [-1, 256, 14, 14]          32,768\n",
      "     BatchNorm2d-755          [-1, 256, 14, 14]             512\n",
      "     BatchNorm2d-756          [-1, 256, 14, 14]             512\n",
      "            ReLU-757          [-1, 256, 14, 14]               0\n",
      "            ReLU-758          [-1, 256, 14, 14]               0\n",
      "      BasicBlock-759          [-1, 256, 14, 14]               0\n",
      "      BasicBlock-760          [-1, 256, 14, 14]               0\n",
      "          Conv2d-761          [-1, 256, 14, 14]         589,824\n",
      "          Conv2d-762          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-763          [-1, 256, 14, 14]             512\n",
      "     BatchNorm2d-764          [-1, 256, 14, 14]             512\n",
      "            ReLU-765          [-1, 256, 14, 14]               0\n",
      "            ReLU-766          [-1, 256, 14, 14]               0\n",
      "          Conv2d-767          [-1, 256, 14, 14]         589,824\n",
      "          Conv2d-768          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-769          [-1, 256, 14, 14]             512\n",
      "     BatchNorm2d-770          [-1, 256, 14, 14]             512\n",
      "            ReLU-771          [-1, 256, 14, 14]               0\n",
      "            ReLU-772          [-1, 256, 14, 14]               0\n",
      "      BasicBlock-773          [-1, 256, 14, 14]               0\n",
      "      BasicBlock-774          [-1, 256, 14, 14]               0\n",
      "          Conv2d-775            [-1, 512, 7, 7]       1,179,648\n",
      "          Conv2d-776            [-1, 512, 7, 7]       1,179,648\n",
      "     BatchNorm2d-777            [-1, 512, 7, 7]           1,024\n",
      "     BatchNorm2d-778            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-779            [-1, 512, 7, 7]               0\n",
      "            ReLU-780            [-1, 512, 7, 7]               0\n",
      "          Conv2d-781            [-1, 512, 7, 7]       2,359,296\n",
      "          Conv2d-782            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-783            [-1, 512, 7, 7]           1,024\n",
      "     BatchNorm2d-784            [-1, 512, 7, 7]           1,024\n",
      "          Linear-785                [-1, 10000]     120,010,000\n",
      "          Conv2d-786            [-1, 512, 7, 7]         131,072\n",
      "          Conv2d-787            [-1, 512, 7, 7]         131,072\n",
      "         Dropout-788                [-1, 10000]               0\n",
      "     BatchNorm2d-789            [-1, 512, 7, 7]           1,024\n",
      "       LeakyReLU-790                [-1, 10000]               0\n",
      "     BatchNorm2d-791            [-1, 512, 7, 7]           1,024\n",
      "          Linear-792                 [-1, 6000]      60,006,000\n",
      "            ReLU-793            [-1, 512, 7, 7]               0\n",
      "            ReLU-794            [-1, 512, 7, 7]               0\n",
      "      BasicBlock-795            [-1, 512, 7, 7]               0\n",
      "      BasicBlock-796            [-1, 512, 7, 7]               0\n",
      "         Dropout-797                 [-1, 6000]               0\n",
      "       LeakyReLU-798                 [-1, 6000]               0\n",
      "          Linear-799                 [-1, 3298]      19,791,298\n",
      "GeoClassification-800                 [-1, 3298]               0\n",
      "          Conv2d-801            [-1, 512, 7, 7]       2,359,296\n",
      "          Conv2d-802            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-803            [-1, 512, 7, 7]           1,024\n",
      "     BatchNorm2d-804            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-805            [-1, 512, 7, 7]               0\n",
      "            ReLU-806            [-1, 512, 7, 7]               0\n",
      "          Conv2d-807            [-1, 512, 7, 7]       2,359,296\n",
      "          Conv2d-808            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-809            [-1, 512, 7, 7]           1,024\n",
      "     BatchNorm2d-810            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-811            [-1, 512, 7, 7]               0\n",
      "            ReLU-812            [-1, 512, 7, 7]               0\n",
      "      BasicBlock-813            [-1, 512, 7, 7]               0\n",
      "      BasicBlock-814            [-1, 512, 7, 7]               0\n",
      "          Linear-815              [-1, 49, 200]         102,600\n",
      "       LeakyReLU-816              [-1, 49, 200]               0\n",
      "         Dropout-817              [-1, 49, 200]               0\n",
      "          Linear-818              [-1, 49, 120]          24,120\n",
      "       LeakyReLU-819              [-1, 49, 120]               0\n",
      "         Dropout-820              [-1, 49, 120]               0\n",
      "          Linear-821               [-1, 49, 80]           9,680\n",
      "       LeakyReLU-822               [-1, 49, 80]               0\n",
      "         Dropout-823               [-1, 49, 80]               0\n",
      "          Linear-824               [-1, 49, 50]           4,050\n",
      "          Linear-825              [-1, 49, 200]         102,600\n",
      "       LeakyReLU-826              [-1, 49, 200]               0\n",
      "         Dropout-827              [-1, 49, 200]               0\n",
      "          Linear-828              [-1, 49, 120]          24,120\n",
      "       LeakyReLU-829              [-1, 49, 120]               0\n",
      "         Dropout-830              [-1, 49, 120]               0\n",
      "          Linear-831               [-1, 49, 80]           9,680\n",
      "       LeakyReLU-832               [-1, 49, 80]               0\n",
      "         Dropout-833               [-1, 49, 80]               0\n",
      "          Linear-834               [-1, 49, 50]           4,050\n",
      "          Linear-835                [-1, 12000]     307,212,000\n",
      "       LeakyReLU-836                [-1, 12000]               0\n",
      "         Dropout-837                [-1, 12000]               0\n",
      "          Linear-838                 [-1, 6000]      72,006,000\n",
      "          Linear-839                [-1, 12000]     307,212,000\n",
      "       LeakyReLU-840                [-1, 12000]               0\n",
      "         Dropout-841                [-1, 12000]               0\n",
      "          Linear-842                 [-1, 6000]      72,006,000\n",
      "          Linear-843                  [-1, 400]       2,400,400\n",
      "       LeakyReLU-844                  [-1, 400]               0\n",
      "         Dropout-845                  [-1, 400]               0\n",
      "          Linear-846                   [-1, 80]          32,080\n",
      "       LeakyReLU-847                   [-1, 80]               0\n",
      "         Dropout-848                   [-1, 80]               0\n",
      "          Linear-849                   [-1, 30]           2,430\n",
      "       LeakyReLU-850                   [-1, 30]               0\n",
      "         Dropout-851                   [-1, 30]               0\n",
      "          Linear-852                   [-1, 10]             310\n",
      "       LeakyReLU-853                   [-1, 10]               0\n",
      "         Dropout-854                   [-1, 10]               0\n",
      "          Linear-855                    [-1, 1]              11\n",
      "          Linear-856                  [-1, 400]       2,400,400\n",
      "       LeakyReLU-857                  [-1, 400]               0\n",
      "         Dropout-858                  [-1, 400]               0\n",
      "          Linear-859                   [-1, 80]          32,080\n",
      "       LeakyReLU-860                   [-1, 80]               0\n",
      "         Dropout-861                   [-1, 80]               0\n",
      "          Linear-862                   [-1, 30]           2,430\n",
      "       LeakyReLU-863                   [-1, 30]               0\n",
      "         Dropout-864                   [-1, 30]               0\n",
      "          Linear-865                   [-1, 10]             310\n",
      "       LeakyReLU-866                   [-1, 10]               0\n",
      "         Dropout-867                   [-1, 10]               0\n",
      "          Linear-868                    [-1, 1]              11\n",
      "          Linear-869                [-1, 10000]     120,010,000\n",
      "         Dropout-870                [-1, 10000]               0\n",
      "       LeakyReLU-871                [-1, 10000]               0\n",
      "          Linear-872                 [-1, 6000]      60,006,000\n",
      "         Dropout-873                 [-1, 6000]               0\n",
      "       LeakyReLU-874                 [-1, 6000]               0\n",
      "          Linear-875                 [-1, 3298]      19,791,298\n",
      "GeoClassification-876                 [-1, 3298]               0\n",
      "================================================================\n",
      "Total params: 2,056,634,056\n",
      "Trainable params: 2,056,634,056\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 86436.00\n",
      "Forward/backward pass size (MB): 640.27\n",
      "Params size (MB): 7845.44\n",
      "Estimated Total Size (MB): 94921.70\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms,models\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from torchsummary import summary\n",
    "import random\n",
    "from sklearn.metrics import f1_score, confusion_matrix, precision_score, recall_score\n",
    "from torchsummary import summary\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"numerical errors at iteration 0\")\n",
    "\n",
    "\n",
    "num_epochs = 600\n",
    "num_classes_coarse = 3298\n",
    "num_classes_middle = 7202\n",
    "num_classes_fine = 12893\n",
    "learning_rate = 0.001\n",
    "\n",
    "multi_hop_dim_1 = 200\n",
    "multi_hop_dim_2 = 120\n",
    "multi_hop_dim_3 = 80\n",
    "multi_hop_dim_4 = 50\n",
    "\n",
    "\n",
    "class GeoClassification(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(GeoClassification, self).__init__()\n",
    "\n",
    "        self.rgb_model = models.resnet34(pretrained=True)  ## load pre-trained weights ##\n",
    "        self.rgb_features = nn.Sequential(*list(self.rgb_model.children())[:-2])\n",
    "        \n",
    "        self.seg_model = models.resnet18(pretrained=True)  ## load pre-trained weights ##\n",
    "        self.seg_features = nn.Sequential(*list(self.seg_model.children())[:-2])\n",
    "        \n",
    "        self.W_s1 = nn.Linear(512, multi_hop_dim_1)\n",
    "        self.W_s2 = nn.Linear(multi_hop_dim_1, multi_hop_dim_2)\n",
    "        self.W_s3 = nn.Linear(multi_hop_dim_2, multi_hop_dim_3)\n",
    "        self.W_s4 = nn.Linear(multi_hop_dim_3, multi_hop_dim_4)\n",
    "        \n",
    "        self.rgb_linear_1 = nn.Linear(multi_hop_dim_4 * 512, 12000)\n",
    "        self.rgb_linear_2 = nn.Linear(12000, 6000)\n",
    "        \n",
    "        self.seg_linear_1 = nn.Linear(multi_hop_dim_4 * 512, 12000)\n",
    "        self.seg_linear_2 = nn.Linear(12000, 6000)\n",
    "        \n",
    "        self.atmf_1 = nn.Linear(6000,400)\n",
    "        self.atmf_2 = nn.Linear(400,80)\n",
    "        self.atmf_3 = nn.Linear(80,30)\n",
    "        self.atmf_4 = nn.Linear(30,10)\n",
    "        self.atmf_5 = nn.Linear(10,1)\n",
    "        \n",
    "        self.fc_concat1 = nn.Linear(12000,10000)\n",
    "        self.fc_concat2 = nn.Linear(10000,6000)\n",
    "        self.fc_concat3 = nn.Linear(6000, num_classes_coarse)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.20)\n",
    "        self.relu = torch.nn.LeakyReLU()\n",
    "        \n",
    "    \n",
    "    def attention_net(self, features):\n",
    "\n",
    "        attn_weight_matrix = self.W_s4(self.dropout(self.relu(self.W_s3(self.dropout(self.relu(self.W_s2(self.dropout(self.relu(self.W_s1(features))))))))))\n",
    "        attn_weight_matrix = attn_weight_matrix.permute(0, 2, 1)\n",
    "        attn_weight_matrix = F.softmax(attn_weight_matrix, dim=2)\n",
    "\n",
    "        return attn_weight_matrix\n",
    "\n",
    "\n",
    "    def forward(self, rgb_image, seg_image):\n",
    "        \n",
    "        seg_image = torch.permute(seg_image, (0,3,1,2))\n",
    "        \n",
    "        rgb_features = self.rgb_features(rgb_image).reshape(-1, 512, 49).permute(0,2,1)\n",
    "        seg_features = self.seg_features(seg_image).reshape(-1, 512, 49).permute(0,2,1)\n",
    "        \n",
    "        rgb_attention_weights = self.attention_net(rgb_features)\n",
    "        seg_attention_weights = self.attention_net(seg_features)\n",
    "\n",
    "        att_rgb_features = torch.bmm(rgb_attention_weights, rgb_features)\n",
    "        att_rgb_features = att_rgb_features.view(-1, att_rgb_features.size()[1] * att_rgb_features.size()[2])\n",
    "\n",
    "        att_seg_features = torch.bmm(seg_attention_weights, seg_features)\n",
    "        att_seg_features = att_seg_features.view(-1, att_seg_features.size()[1] * att_seg_features.size()[2])\n",
    "        \n",
    "        att_rgb_features = self.rgb_linear_2(self.dropout(self.relu(self.rgb_linear_1(att_rgb_features))))\n",
    "        att_seg_features = self.seg_linear_2(self.dropout(self.relu(self.seg_linear_1(att_seg_features))))\n",
    "        \n",
    "        \n",
    "        s_rgb = self.atmf_5(self.dropout(self.relu(self.atmf_4(self.dropout(self.relu(self.atmf_3(self.dropout(self.relu(self.atmf_2(self.dropout(self.relu(self.atmf_1(att_rgb_features)))))))))))))\n",
    "        s_seg = self.atmf_5(self.dropout(self.relu(self.atmf_4(self.dropout(self.relu(self.atmf_3(self.dropout(self.relu(self.atmf_2(self.dropout(self.relu(self.atmf_1(att_seg_features)))))))))))))\n",
    "\n",
    "        s_comb = torch.cat((s_rgb, s_seg), 0)\n",
    "        s_comb = F.softmax(s_comb, dim=0) + 1\n",
    "        att_rgb_features = torch.mul(att_rgb_features, s_comb[0].item())\n",
    "        att_seg_features = torch.mul(att_seg_features, s_comb[1].item())\n",
    "\n",
    "        concat_embed = torch.cat((att_rgb_features, att_seg_features),1)\n",
    "        \n",
    "        concat_embed = self.fc_concat1(concat_embed)\n",
    "        concat_embed = self.dropout(concat_embed)\n",
    "        concat_embed = self.relu(concat_embed)\n",
    "\n",
    "\n",
    "        concat_embed = self.fc_concat2(concat_embed)\n",
    "        concat_embed = self.dropout(concat_embed)\n",
    "        concat_embed = self.relu(concat_embed)\n",
    "\n",
    "        \n",
    "        output = self.fc_concat3(concat_embed)\n",
    "        \n",
    "        return (output)\n",
    "    \n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GeoClassification()     \n",
    "model = model.to(device)\n",
    "model = nn.DataParallel(model)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "step_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "print(summary(model, [(3, 224, 224), (224, 224, 3)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5128bcfc",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 458.00 MiB (GPU 0; 23.65 GiB total capacity; 21.51 GiB already allocated; 362.56 MiB free; 21.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-12b09b378730>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# Backward and optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mstep_lr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/glimpse/classification_network/glimpse_classification_env/lib/python3.6/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    272\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/glimpse/classification_network/glimpse_classification_env/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/glimpse/classification_network/glimpse_classification_env/lib/python3.6/site-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;31m# _forward_cls is defined by derived class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_cls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/glimpse/classification_network/glimpse_classification_env/lib/python3.6/site-packages/torch/nn/parallel/_functions.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(ctx, *grad_outputs)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mgrad_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mReduceAddCoalesced\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mgrad_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/glimpse/classification_network/glimpse_classification_env/lib/python3.6/site-packages/torch/nn/parallel/_functions.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, destination, num_inputs, *grads)\u001b[0m\n\u001b[1;32m     43\u001b[0m         grads_ = [grads[i:i + num_inputs]\n\u001b[1;32m     44\u001b[0m                   for i in range(0, len(grads), num_inputs)]\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcomm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_add_coalesced\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestination\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/glimpse/classification_network/glimpse_classification_env/lib/python3.6/site-packages/torch/nn/parallel/comm.py\u001b[0m in \u001b[0;36mreduce_add_coalesced\u001b[0;34m(inputs, destination, buffer_size)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mchunks\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mitrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mflat_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_flatten_dense_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# (num_gpus,)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0mflat_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestination\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_unflatten_dense_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0;31m# The unflattened tensors do not share storage, and we don't expose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/glimpse/classification_network/glimpse_classification_env/lib/python3.6/site-packages/torch/nn/parallel/comm.py\u001b[0m in \u001b[0;36mreduce_add\u001b[0;34m(inputs, destination)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnccl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mroot_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0mnccl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mroot_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 458.00 MiB (GPU 0; 23.65 GiB total capacity; 21.51 GiB already allocated; 362.56 MiB free; 21.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "n_total_steps = len(train_data_loader)\n",
    "\n",
    "batch_wise_loss = []\n",
    "batch_wise_micro_f1 = []\n",
    "batch_wise_macro_f1 = []\n",
    "epoch_wise_macro_f1 = []\n",
    "epoch_wise_micro_f1 = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (rgb_image, seg_image, label, _, _, _) in enumerate(train_data_loader):\n",
    "        rgb_image = rgb_image.type(torch.float32).to(device)\n",
    "        seg_image = seg_image.type(torch.float32).to(device)\n",
    "\n",
    "        label = label[0].to(device)\n",
    "        \n",
    "         # Forward pass\n",
    "        model.train()\n",
    "        outputs = model(rgb_image, seg_image)\n",
    "        loss = criterion(outputs, label)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        step_lr_scheduler.step()\n",
    "        \n",
    "        if (i+1) % 20 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "     \n",
    "    target_total_test = []\n",
    "    predicted_total_test = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        n_correct = 0\n",
    "        n_samples = 0\n",
    "\n",
    "        for i, (rgb_image, seg_image, label, _, _, _) in enumerate(val_data_loader):\n",
    "            \n",
    "            rgb_image = rgb_feature.type(torch.float32).to(device)\n",
    "            seg_image = seg_image.type(torch.float32).to(device)\n",
    "\n",
    "            label = label[0].to(device)\n",
    "\n",
    "\n",
    "            # Forward pass\n",
    "            model.eval()\n",
    "            outputs = model(rgb_image, seg_image)\n",
    "            #print(outputs)\n",
    "            # max returns (value ,index)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            #print(label)\n",
    "            #print(predicted)\n",
    "            n_samples += label.size(0)\n",
    "            n_correct += (predicted == label).sum().item()\n",
    "\n",
    "            target_total_test.append(label)\n",
    "            predicted_total_test.append(predicted)\n",
    "\n",
    "            target_inter = [t.cpu().numpy() for t in target_total_test]\n",
    "            predicted_inter = [t.cpu().numpy() for t in predicted_total_test]\n",
    "            target_inter =  np.stack(target_inter, axis=0).ravel()\n",
    "            predicted_inter =  np.stack(predicted_inter, axis=0).ravel()\n",
    "\n",
    "        current_macro = f1_score(target_inter, predicted_inter, average=\"macro\")\n",
    "        epoch_wise_macro_f1.append(f1_score(target_inter, predicted_inter, average=\"macro\"))\n",
    "        epoch_wise_micro_f1.append(f1_score(target_inter, predicted_inter, average=\"micro\"))\n",
    "\n",
    "        \n",
    "        acc = 100.0 * n_correct / n_samples\n",
    "        print(f'Accuracy of the network on the test set after Epoch {epoch+1} is: {acc} %')        \n",
    "        print(f' Micro F1 on the testing: {f1_score(target_inter, predicted_inter, average=\"micro\")}')\n",
    "        print(f' Macro F1 on the testing: {f1_score(target_inter, predicted_inter, average=\"macro\")}')\n",
    "        print(f' Precision on the testing: {precision_score(target_inter, predicted_inter)}')\n",
    "        print(f' Recall on the testing: {recall_score(target_inter, predicted_inter)}')\n",
    "        print(confusion_matrix(target_inter, predicted_inter))   \n",
    "        print(f'Best Macro F1 on test set till this epoch: {max(epoch_wise_macro_f1)} Found in Epoch No: {epoch_wise_macro_f1.index(max(epoch_wise_macro_f1))+1}')\n",
    "        print(f'Best Micro F1 on test set till this epoch: {max(epoch_wise_micro_f1)} Found in Epoch No: {epoch_wise_micro_f1.index(max(epoch_wise_micro_f1))+1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8be9dc35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import top_k_accuracy_score\n",
    "import torch\n",
    "\n",
    "def topk_accuracy(target, output, k):\n",
    "    topn = np.argsort(output, axis = 1)[:,-k:]\n",
    "    return np.mean(np.array([1 if target[k] in topn[k] else 0 for k in range(len(topn))]))\n",
    "\n",
    "y_true = np.array([0, 1, 2, 2])\n",
    "y_score = np.array([[0.5, 0.2, 0.2],  # 0 is in top 2\n",
    "                    [0.3, 0.4, 0.2],  # 1 is in top 2\n",
    "                    [0.2, 0.4, 0.3],  # 2 is in top 2\n",
    "                    [0.7, 0.2, 0.1]]) # 2 isn't in top 2\n",
    "print(topk_accuracy(y_true, y_score, k = 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80683864",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glimpse_classification_env",
   "language": "python",
   "name": "glimpse_classification_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
